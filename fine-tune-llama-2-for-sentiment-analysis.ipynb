{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1192499,"sourceType":"datasetVersion","datasetId":622510},{"sourceId":4295,"sourceType":"modelInstanceVersion","modelInstanceId":3090}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Fine-tune Llama 2 for Sentiment Analysis\n\n\nUsing Llama2 model on financial dataset available on kaggle to demonstrate the fine tuning process","metadata":{}},{"cell_type":"markdown","source":"Install the specific libraries necessary to make this work","metadata":{}},{"cell_type":"markdown","source":"#### Main libraries to be used\n* accelerate(req for training models in pytorch & Huggingface)\n* peft(performance efficient fine tuning)\n* bitsandbytes(for quantization)\n* transformers\n* trl(hf library for modelling)","metadata":{}},{"cell_type":"markdown","source":"#### Installations and imports","metadata":{}},{"cell_type":"code","source":"!pip install -q -U \"torch==2.1.2\" tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:05:07.976178Z","iopub.execute_input":"2024-03-30T04:05:07.976498Z","iopub.status.idle":"2024-03-30T04:07:30.608156Z","shell.execute_reply.started":"2024-03-30T04:05:07.976470Z","shell.execute_reply":"2024-03-30T04:07:30.607013Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.1.2 which is incompatible.\ntensorflow 2.12.0 requires tensorboard<2.13,>=2.12, but you have tensorboard 2.16.2 which is incompatible.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Using specific versions for these libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\"","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:07:42.522908Z","iopub.execute_input":"2024-03-30T04:07:42.523299Z","iopub.status.idle":"2024-03-30T04:08:11.667140Z","shell.execute_reply.started":"2024-03-30T04:07:42.523268Z","shell.execute_reply":"2024-03-30T04:08:11.666039Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The code imports the os module and sets two environment variables:\n* CUDA_VISIBLE_DEVICES: tells which GPU to use\n* TOKENIZERS_PARALLELISM: set to false","metadata":{}},{"cell_type":"code","source":"!pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:08:19.752632Z","iopub.execute_input":"2024-03-30T04:08:19.753003Z","iopub.status.idle":"2024-03-30T04:09:16.432078Z","shell.execute_reply.started":"2024-03-30T04:08:19.752975Z","shell.execute_reply":"2024-03-30T04:09:16.430519Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:09:19.567083Z","iopub.execute_input":"2024-03-30T04:09:19.567957Z","iopub.status.idle":"2024-03-30T04:09:19.572727Z","shell.execute_reply.started":"2024-03-30T04:09:19.567909Z","shell.execute_reply":"2024-03-30T04:09:19.571745Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"The code import warnings; warnings.filterwarnings(\"ignore\") imports the warnings module and sets the warning filter to ignore. This means that all warnings will be suppressed and will not be displayed. Actually during training there are many warnings that do not prevent the fine-tuning but can be distracting and make you wonder if you are doing the correct things.","metadata":{}},{"cell_type":"markdown","source":"In the following cell there are all the other imports for running the notebook","metadata":{}},{"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm # provides a progress bar to see the execution\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig,TrainingArguments,pipeline,logging\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":14.485002,"end_time":"2023-10-16T11:00:18.917449","exception":false,"start_time":"2023-10-16T11:00:04.432447","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-30T04:09:24.486975Z","iopub.execute_input":"2024-03-30T04:09:24.487327Z","iopub.status.idle":"2024-03-30T04:09:40.251721Z","shell.execute_reply.started":"2024-03-30T04:09:24.487300Z","shell.execute_reply":"2024-03-30T04:09:40.250908Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Preparing the data and the core evaluation functions","metadata":{}},{"cell_type":"markdown","source":"#### The code in the next cell performs the following steps:\n1. Read data,split dataset into training & test datasets\n2. Shuffle the data\n3. Transform text in train & test set into prompts to be used by Llama2\n4. train & eval data needs to be wrapped into datasets class","metadata":{}},{"cell_type":"code","source":"filename = \"../input/sentiment-analysis-for-financial-news/all-data.csv\"\n\ndf = pd.read_csv(filename, \n                 names=[\"sentiment\", \"text\"],\n                 encoding=\"utf-8\", encoding_errors=\"replace\")\n\nX_train = list()\nX_test = list()\nfor sentiment in [\"positive\", \"neutral\", \"negative\"]:\n    train, test  = train_test_split(df[df.sentiment==sentiment], \n                                    train_size=300,\n                                    test_size=300, \n                                    random_state=42)\n    X_train.append(train)\n    X_test.append(test)\n\nX_train = pd.concat(X_train).sample(frac=1, random_state=10)\nX_test = pd.concat(X_test)\n\neval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\nX_eval = df[df.index.isin(eval_idx)]\nX_eval = (X_eval\n          .groupby('sentiment', group_keys=False)\n          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\nX_train = X_train.reset_index(drop=True)\n\ndef generate_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n            \"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = \"\"\".strip()\n\nX_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n                       columns=[\"text\"])\nX_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n                      columns=[\"text\"])\n\ny_true = X_test.sentiment\nX_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\ntrain_data = Dataset.from_pandas(X_train)\neval_data = Dataset.from_pandas(X_eval)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:09:50.842554Z","iopub.execute_input":"2024-03-30T04:09:50.843398Z","iopub.status.idle":"2024-03-30T04:09:52.156741Z","shell.execute_reply.started":"2024-03-30T04:09:50.843366Z","shell.execute_reply":"2024-03-30T04:09:52.155945Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Next we create a function to evaluate the results from our fine-tuned sentiment model. The function performs the following steps:\n\n1. Maps the sentiment labels to a numerical representation, where 2 represents positive, 1 represents neutral, and 0 represents negative.\n2. Calculates the accuracy of the model on the test data.\n3. Generates an accuracy report for each sentiment label.\n4. Generates a classification report for the model.\n5. Generates a confusion matrix for the model.","metadata":{}},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    labels = ['positive', 'neutral', 'negative']\n    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n    def map_func(x):\n        return mapping.get(x, 1)\n    \n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    # Generate accuracy report\n    unique_labels = set(y_true)  # Get unique labels\n    \n    for label in unique_labels:\n        label_indices = [i for i in range(len(y_true)) \n                         if y_true[i] == label]\n        label_y_true = [y_true[i] for i in label_indices]\n        label_y_pred = [y_pred[i] for i in label_indices]\n        accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label {label}: {accuracy:.3f}')\n        \n    # Generate classification report\n    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n    print('\\nClassification Report:')\n    print(class_report)\n    \n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:09:55.194593Z","iopub.execute_input":"2024-03-30T04:09:55.195434Z","iopub.status.idle":"2024-03-30T04:09:55.204231Z","shell.execute_reply.started":"2024-03-30T04:09:55.195397Z","shell.execute_reply":"2024-03-30T04:09:55.203286Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Testing the model without fine-tuning","metadata":{}},{"cell_type":"code","source":"model_name = \"../input/llama-2/pytorch/7b-hf/1\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          trust_remote_code=True,\n                                         )\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:12:00.496090Z","iopub.execute_input":"2024-03-30T04:12:00.496830Z","iopub.status.idle":"2024-03-30T04:14:54.334191Z","shell.execute_reply.started":"2024-03-30T04:12:00.496796Z","shell.execute_reply":"2024-03-30T04:14:54.333319Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"549cb2696c49478a97f125a920ab5e83"}},"metadata":{}}]},{"cell_type":"code","source":"def predict(test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"text\"]\n        pipe = pipeline(task=\"text-generation\", \n                        model=model, \n                        tokenizer=tokenizer, \n                        max_new_tokens = 1, \n                        temperature = 0.0,\n                       )\n        result = pipe(prompt)\n        answer = result[0]['generated_text'].split(\"=\")[-1]\n        if \"positive\" in answer:\n            y_pred.append(\"positive\")\n        elif \"negative\" in answer:\n            y_pred.append(\"negative\")\n        elif \"neutral\" in answer:\n            y_pred.append(\"neutral\")\n        else:\n            y_pred.append(\"none\")\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:23:38.974963Z","iopub.execute_input":"2024-03-30T04:23:38.976035Z","iopub.status.idle":"2024-03-30T04:23:38.982901Z","shell.execute_reply.started":"2024-03-30T04:23:38.976000Z","shell.execute_reply":"2024-03-30T04:23:38.981973Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"y_pred = predict(test, model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:23:44.371730Z","iopub.execute_input":"2024-03-30T04:23:44.372103Z","iopub.status.idle":"2024-03-30T04:29:12.820033Z","shell.execute_reply.started":"2024-03-30T04:23:44.372072Z","shell.execute_reply":"2024-03-30T04:29:12.819084Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 900/900 [05:28<00:00,  2.74it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:29:18.610616Z","iopub.execute_input":"2024-03-30T04:29:18.611046Z","iopub.status.idle":"2024-03-30T04:29:18.637491Z","shell.execute_reply.started":"2024-03-30T04:29:18.611012Z","shell.execute_reply":"2024-03-30T04:29:18.636363Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Accuracy: 0.373\nAccuracy for label 0: 0.027\nAccuracy for label 1: 0.937\nAccuracy for label 2: 0.157\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.03      0.05       300\n           1       0.34      0.94      0.50       300\n           2       0.67      0.16      0.25       300\n\n    accuracy                           0.37       900\n   macro avg       0.63      0.37      0.27       900\nweighted avg       0.63      0.37      0.27       900\n\n\nConfusion Matrix:\n[[  8 287   5]\n [  1 281  18]\n [  0 253  47]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fine-tuning","metadata":{}},{"cell_type":"code","source":"output_dir=\"trained_weigths\"\n\npeft_config = LoraConfig(\n        lora_alpha=16, \n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\",\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,                    # directory to save and repository id\n    num_train_epochs=1,                       # number of training epochs\n    per_device_train_batch_size=1,            # batch size per device during training\n    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,                         # log every 10 steps\n    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n    max_steps=-1,\n    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n    report_to=\"tensorboard\",                  # report metrics to tensorboard\n    evaluation_strategy=\"epoch\"               # save checkpoint every epoch\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=1024,\n    packing=False,\n    dataset_kwargs={\n        \"add_special_tokens\": False,\n        \"append_concat_token\": False,\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:29:33.451380Z","iopub.execute_input":"2024-03-30T04:29:33.451752Z","iopub.status.idle":"2024-03-30T04:29:36.324107Z","shell.execute_reply.started":"2024-03-30T04:29:33.451722Z","shell.execute_reply":"2024-03-30T04:29:36.323321Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21161a93a99c4d23807eab2d4c2fac65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e015d9d5e35e40fba9d18d379204a12a"}},"metadata":{}}]},{"cell_type":"markdown","source":"The following code will train the model using the trainer.train() method and then save the trained model to the trained-model directory. Using The standard GPU P100 offered by Kaggle, the training should be quite fast.","metadata":{}},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:29:53.855087Z","iopub.execute_input":"2024-03-30T04:29:53.855842Z","iopub.status.idle":"2024-03-30T04:50:25.525404Z","shell.execute_reply.started":"2024-03-30T04:29:53.855805Z","shell.execute_reply":"2024-03-30T04:50:25.524527Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='112' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [112/112 20:17, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.802200</td>\n      <td>0.713373</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=112, training_loss=0.9133915816034589, metrics={'train_runtime': 1231.0927, 'train_samples_per_second': 0.731, 'train_steps_per_second': 0.091, 'total_flos': 3569339123662848.0, 'train_loss': 0.9133915816034589, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"The model and the tokenizer are saved to disk for later usage.","metadata":{}},{"cell_type":"code","source":"# Save trained model and tokenizer\ntrainer.save_model()\ntokenizer.save_pretrained(output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:50:30.751353Z","iopub.execute_input":"2024-03-30T04:50:30.751731Z","iopub.status.idle":"2024-03-30T04:50:34.842390Z","shell.execute_reply.started":"2024-03-30T04:50:30.751698Z","shell.execute_reply":"2024-03-30T04:50:34.841353Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"('trained_weigths/tokenizer_config.json',\n 'trained_weigths/special_tokens_map.json',\n 'trained_weigths/tokenizer.model',\n 'trained_weigths/added_tokens.json',\n 'trained_weigths/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Saving model to disk for later usage","metadata":{}},{"cell_type":"markdown","source":"Before proceeding, we first remove the previous model and clean up the memory from various objects we won't use anymore.","metadata":{}},{"cell_type":"code","source":"import gc\n\ndel [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\ndel [df, X_train, X_eval]\ndel [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:50:55.324295Z","iopub.execute_input":"2024-03-30T04:50:55.325332Z","iopub.status.idle":"2024-03-30T04:50:55.331473Z","shell.execute_reply.started":"2024-03-30T04:50:55.325284Z","shell.execute_reply":"2024-03-30T04:50:55.330417Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"for _ in range(100):\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:51:02.643859Z","iopub.execute_input":"2024-03-30T04:51:02.644249Z","iopub.status.idle":"2024-03-30T04:51:35.913286Z","shell.execute_reply.started":"2024-03-30T04:51:02.644218Z","shell.execute_reply":"2024-03-30T04:51:35.912309Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Then we can proceed to merging the weights and we will be using the merged model for our testing purposes.","metadata":{}},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\nfinetuned_model = \"./trained_weigths/\"\ncompute_dtype = getattr(torch, \"float16\")\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-2/pytorch/7b-hf/1\")\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n     finetuned_model,\n     torch_dtype=compute_dtype,\n     return_dict=False,\n     low_cpu_mem_usage=True,\n     device_map=device,\n)\n\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./merged_model\",safe_serialization=True, max_shard_size=\"2GB\")\ntokenizer.save_pretrained(\"./merged_model\")","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:51:50.936232Z","iopub.execute_input":"2024-03-30T04:51:50.936946Z","iopub.status.idle":"2024-03-30T04:52:44.653538Z","shell.execute_reply.started":"2024-03-30T04:51:50.936888Z","shell.execute_reply":"2024-03-30T04:52:44.652662Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb2b5b53cc1b45fbbe7578f4e1a9183e"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"('./merged_model/tokenizer_config.json',\n './merged_model/special_tokens_map.json',\n './merged_model/tokenizer.model',\n './merged_model/added_tokens.json',\n './merged_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"markdown","source":"The following code will first predict the sentiment labels for the test set using the predict() function. Then, it will evaluate the model's performance on the test set using the evaluate() function. The result now should be impressive with an overall accuracy of over 0.8 and high accuracy, precision and recall for the single sentiment labels. The prediction of the neutral label can still be improved, yet it is impressive how much could be done with little data and some fine-tuning.","metadata":{}},{"cell_type":"code","source":"y_pred = predict(test, merged_model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:53:00.932380Z","iopub.execute_input":"2024-03-30T04:53:00.932784Z","iopub.status.idle":"2024-03-30T04:56:51.584804Z","shell.execute_reply.started":"2024-03-30T04:53:00.932753Z","shell.execute_reply":"2024-03-30T04:56:51.583849Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"100%|██████████| 900/900 [03:50<00:00,  3.90it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.808\nAccuracy for label 0: 0.967\nAccuracy for label 1: 0.617\nAccuracy for label 2: 0.840\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.90      0.97      0.93       300\n           1       0.79      0.62      0.69       300\n           2       0.73      0.84      0.78       300\n\n    accuracy                           0.81       900\n   macro avg       0.81      0.81      0.80       900\nweighted avg       0.81      0.81      0.80       900\n\n\nConfusion Matrix:\n[[290   8   2]\n [ 24 185  91]\n [  8  40 252]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The following code will create a Pandas DataFrame called evaluation containing the text, true labels, and predicted labels from the test set. This is expectially useful for understanding the errors that the fine-tuned model makes, and gettting insights on how to improve the prompt.","metadata":{}},{"cell_type":"code","source":"evaluation = pd.DataFrame({'text': X_test[\"text\"], \n                           'y_true':y_true, \n                           'y_pred': y_pred},\n                         )\nevaluation.to_csv(\"test_predictions.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T04:57:00.378111Z","iopub.execute_input":"2024-03-30T04:57:00.378489Z","iopub.status.idle":"2024-03-30T04:57:00.415072Z","shell.execute_reply.started":"2024-03-30T04:57:00.378460Z","shell.execute_reply":"2024-03-30T04:57:00.414154Z"},"trusted":true},"execution_count":24,"outputs":[]}]}